# -------------------------------
# 1. IMPORT LIBRARIES
# -------------------------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Scikit-learn modules
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import f1_score, classification_report, confusion_matrix
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# XGBoost and SMOTE
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE

# To ignore warnings
import warnings
warnings.filterwarnings("ignore")

# -------------------------------
# 2. READ AND PREPARE DATA
# -------------------------------
# Read data from Excel (update path accordingly)
file_path = 'complaints.xlsx'
df = pd.read_excel(file_path)

# Clean column names (remove leading/trailing spaces)
df.columns = df.columns.str.strip()

# Define your categorical columns as per your shared images
category_cols = ["Account Type", "Channel", "Primary_Complaint_Reason", 
                 "Primary_Complaint_Category", "LOB_About_Product"]

# Remove extra spaces in the values and replace inner spaces with underscores
for col in category_cols:
    df[col] = df[col].astype(str).str.strip().str.replace(" ", "_")

# Ensure there are no NaNs in categorical columns
df[category_cols] = df[category_cols].fillna("Unknown")

# -------------------------------
# 3. TARGET VARIABLE AND FEATURE ENGINEERING
# -------------------------------
# Map target variable (assuming "Complaint Escalated" uses 'Yes'/'No')
df["Complaint_Escalated_Flag"] = df["Complaint Escalated"].map({"Yes": 1, "No": 0})

# --- Create new features based on definitions ---
# 1. Escalation_to_Resolution_Ratio:
#    Use "Escalation History" (number of escalations) and "Resolution Time (Days)"
df["Escalation_to_Resolution_Ratio"] = df["Escalation History"] / (df["Resolution Time (Days)"] + 1)

# 2. Satisfaction_vs_Complaint_Length:
#    Use "Satisfaction Score" and "Complaint Length"
df["Satisfaction_vs_Complaint_Length"] = df["Satisfaction Score"] / (df["Complaint Length"] + 1)

# Optionally, you can log-transform these new features to reduce skewness:
df["Escalation_to_Resolution_Ratio"] = np.log1p(df["Escalation_to_Resolution_Ratio"])
df["Satisfaction_vs_Complaint_Length"] = np.log1p(df["Satisfaction_vs_Complaint_Length"])

# Optionally drop low importance features (e.g., "Age") if prior analysis suggests
# For example, if "Age" has low importance, uncomment the next line:
# df.drop(columns=["Age"], inplace=True)

# Define the list of numeric columns (including new features)
numeric_cols = ["Complaint Length", "Previous Complaints", "Resolution Time (Days)", 
                "Satisfaction Score", "Escalation History", 
                "Escalation_to_Resolution_Ratio", "Satisfaction_vs_Complaint_Length"]

# Define full feature list (combine numeric and categorical features)
# You can adjust these lists based on your domain knowledge and previous feature importance analysis.
features = numeric_cols + category_cols

# Define target column
target = "Complaint_Escalated_Flag"

# -------------------------------
# 4. SPLIT DATA INTO TRAIN/TEST
# -------------------------------
X = df[features].copy()
y = df[target].copy()

X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size=0.2, 
                                                    stratify=y, 
                                                    random_state=42)

# -------------------------------
# 5. PREPROCESSING PIPELINE
# -------------------------------
# Preprocessing for numeric features: impute missing values and scale
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# Preprocessing for categorical features: One-Hot Encode (drop first to avoid collinearity)
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value="Unknown")),
    ('onehot', OneHotEncoder(sparse=False, drop='first'))
])

# Combine preprocessing steps
preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numeric_cols),
    ('cat', categorical_transformer, category_cols)
])

# -------------------------------
# 6. BUILD FULL PIPELINE WITH XGBoost
# -------------------------------
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('clf', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'))
])

# -------------------------------
# 7. APPLY SMOTE TO TRAINING DATA
# -------------------------------
# We apply SMOTE AFTER splitting and before fitting.
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# -------------------------------
# 8. HYPERPARAMETER TUNING WITH GRID SEARCH
# -------------------------------
param_grid = {
    'clf__n_estimators': [100, 150],
    'clf__max_depth': [3, 6, 9],
    'clf__learning_rate': [0.01, 0.1, 0.2],
    'clf__subsample': [0.7, 1.0],
    'clf__colsample_bytree': [0.7, 1.0]
}

grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='f1', n_jobs=-1, verbose=1)
grid_search.fit(X_train_res, y_train_res)

print("Best Parameters:", grid_search.best_params_)

# -------------------------------
# 9. EVALUATE MODEL ON TEST DATA
# -------------------------------
y_pred = grid_search.predict(X_test)
f1 = f1_score(y_test, y_pred)
print("Test F1-Score: {:.2f}".format(f1))
print(classification_report(y_test, y_pred))

# Plot confusion matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# -------------------------------
# 10. OPTIONAL: SAVE THE FINAL MODEL
# -------------------------------
import pickle
with open("final_escalation_model.pkl", "wb") as f:
    pickle.dump(grid_search.best_estimator_, f)
print("Final model saved.")
