# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, classification_report
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import pickle

# Load the dataset
df = pd.read_csv('complaint_data.csv')

# Convert the target variable to binary (1 for 'Yes', 0 for 'No')
df['P_Complaint_Escalated'] = (df['P_Complaint_Escalated'] == 'Yes').astype(int)

# Feature engineering: Extract features from RECEIVED_DATE
df['RECEIVED_DATE'] = pd.to_datetime(df['RECEIVED_DATE'])
df['Day_of_Week'] = df['RECEIVED_DATE'].dt.dayofweek  # 0-6 (Monday-Sunday)
df['Month'] = df['RECEIVED_DATE'].dt.month  # 1-12
df['Is_Weekend'] = df['RECEIVED_DATE'].dt.dayofweek.isin([5, 6]).astype(int)  # 1 if weekend, 0 otherwise

# Define feature columns
numerical_cols = [
    'Age', 
    'Complaint Length', 
    'Previous Complaints', 
    'Resolution Time (Days)', 
    'Satisfaction Score', 
    'Is_Weekend'
]
categorical_cols = [
    'Account Type', 
    'Channel', 
    'Escalation History', 
    'Primary Complaint Reason', 
    'PRIMARY_COMPLAINT_CATEGORY', 
    'Day_of_Week', 
    'Month'
]

# Define the preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),  # Scale numerical features
        ('cat', OneHotEncoder(drop='if_binary', handle_unknown='ignore'), categorical_cols)  # Encode categorical features
    ]
)

# Create a pipeline with SMOTE and RandomForestClassifier
pipeline = ImbPipeline([
    ('preprocessor', preprocessor),
    ('smote', SMOTE(random_state=42)),  # Handle class imbalance
    ('classifier', RandomForestClassifier(random_state=42))  # Classifier
])

# Define features (X) and target (y)
X = df[numerical_cols + categorical_cols]
y = df['P_Complaint_Escalated']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Define hyperparameter grid for tuning
param_grid = {
    'classifier__n_estimators': [100, 200],  # Number of trees
    'classifier__max_depth': [10, 20, None]  # Maximum depth of trees
}

# Perform grid search with cross-validation to maximize F1 score
grid_search = GridSearchCV(
    pipeline, 
    param_grid, 
    cv=5, 
    scoring='f1', 
    n_jobs=-1
)
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Make predictions on the test set
y_pred = best_model.predict(X_test)

# Evaluate the model
f1 = f1_score(y_test, y_pred)
print(f"F1 Score on Test Set: {f1:.3f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Save the trained model to a file
with open('best_model.pkl', 'wb') as f:
    pickle.dump(best_model, f)

# Generate predictions for the entire dataset
df['Predicted_Escalation'] = best_model.predict(X)

# Save the dataset with predictions to an Excel file
df.to_excel('predictions.xlsx', index=False)

print("Model saved as 'best_model.pkl' and predictions saved as 'predictions.xlsx'.")
