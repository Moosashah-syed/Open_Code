#############################################
# 1. IMPORT LIBRARIES
#############################################
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Scikit-learn modules
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, confusion_matrix
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# For handling imbalance
from imblearn.over_sampling import SMOTE

# For saving the model
import pickle

#############################################
# 2. READ DATA FROM EXCEL
#############################################
# Replace 'complaints.xlsx' with your Excel file path
file_path = 'complaints.xlsx'
df = pd.read_excel(file_path)

# Quick check
print("Data sample:")
display(df.head())
print("Data shape:", df.shape)
print(df.info())

#############################################
# 3. DATA CLEANING & PREPROCESSING
#############################################
# Example: convert date columns if needed
# df['Ticket_Received_Date'] = pd.to_datetime(df['Ticket_Received_Date'])
# df['Ticket_Closed_Date'] = pd.to_datetime(df['Ticket_Closed_Date'])

# Remove rows that are completely empty
df.dropna(how='all', inplace=True)

# Map the target column to binary (adjust mapping based on your actual values)
df['Complaint_Escalated_Flag'] = df['Complaint Escalated'].map({'Yes': 1, 'No': 0})

# Quick check of target distribution (imbalanced expected)
print("Target distribution:")
print(df['Complaint_Escalated_Flag'].value_counts())

#############################################
# 4. FEATURE ENGINEERING & SELECTION
#############################################
# List the features you want to use â€“ adjust these names as needed.
feature_cols = [
    'Age',
    'Account Type',
    'Complaint Length',
    'Previous Complaints',
    'Resolution Time (days)',
    'Satisfaction Score',
    'Channel',
    'Escalation History',
    'Primary_Complaint_Reason',
    'Primary_Complaint_Category',
    'LOB_About_Complaint'
]

target_col = 'Complaint_Escalated_Flag'

# Create the modeling dataframe
df_model = df[feature_cols + [target_col]].copy()

#############################################
# 5. SPLIT THE DATA INTO TRAIN AND TEST SETS
#############################################
X = df_model[feature_cols]
y = df_model[target_col]

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

#############################################
# 6. SET UP THE PREPROCESSING PIPELINE
#############################################
# Define which features are numeric vs. categorical
numeric_features = ['Age', 'Complaint Length', 'Previous Complaints', 'Resolution Time (days)', 'Satisfaction Score']
categorical_features = ['Account Type', 'Channel', 'Escalation History', 'Primary_Complaint_Reason', 
                        'Primary_Complaint_Category', 'LOB_About_Complaint']

# Pipeline for numeric features: impute missing values with median then scale
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# Pipeline for categorical features: impute missing values with most frequent then one-hot encode
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine both transformations
preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])

#############################################
# 7. HANDLE CLASS IMBALANCE (SMOTE)
#############################################
# Apply SMOTE to only the training data
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
print("\nAfter SMOTE:")
print("Resampled target distribution:", np.bincount(y_train_resampled))

#############################################
# 8. BUILD THE MODEL PIPELINE
#############################################
# Here we use a RandomForestClassifier with class_weight as an additional measure.
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('clf', RandomForestClassifier(class_weight='balanced', random_state=42))
])

# Optionally, you could use cross-validation to check performance on the training set:
# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
# cv_scores = cross_val_score(pipeline, X_train_resampled, y_train_resampled, cv=skf, scoring='f1')
# print("Cross-validated F1 scores:", cv_scores)
# print("Mean CV F1-score:", np.mean(cv_scores))

#############################################
# 9. TRAIN THE MODEL
#############################################
pipeline.fit(X_train_resampled, y_train_resampled)

#############################################
# 10. EVALUATE THE MODEL ON THE TEST SET
#############################################
y_pred = pipeline.predict(X_test)

# Calculate metrics
f1 = f1_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

print("\nTest Set Evaluation:")
print("F1-score:", f1)
print("Precision:", precision)
print("Recall:", recall)
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Plotting the confusion matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

#############################################
# 11. SAVE THE TRAINED MODEL
#############################################
model_filename = 'final_escalation_model.pkl'
with open(model_filename, 'wb') as f:
    pickle.dump(pipeline, f)
print(f"Model saved to {model_filename}")

#############################################
# 12. GENERATE AND SAVE PREDICTIONS (OPTIONAL)
#############################################
# For example, saving predictions from the test set:
test_results = X_test.copy()
test_results['Actual_Escalation'] = y_test
test_results['Predicted_Escalation'] = y_pred

# Save as an Excel file (or you can use to_csv() to save as CSV)
test_results.to_excel('test_predictions.xlsx', index=False)
print("Test set predictions saved to 'test_predictions.xlsx'")
